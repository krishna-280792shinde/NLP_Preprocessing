{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be097be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The Moon is a barren, rocky world without air and water.\n",
    "It has dark lava plain on its surface. The Moon is filled wit craters. \n",
    "It has no light of its own. It gets its light from the Sun. The Moo keeps \n",
    "changing its shape as it moves round the Earth. It spins on its axis in 27.3 \n",
    "days stars were named after the Edwin Aldrin were the first ones to set their \n",
    "foot on the Moon on 21 July 1969 They reached the Moon in their space craft named Apollo II.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c1749",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fed4cb",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eee9a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Moon is a barren, rocky world without air and water.',\n",
       " 'It has dark lava plain on its surface.',\n",
       " 'The Moon is filled wit craters.',\n",
       " 'It has no light of its own.',\n",
       " 'It gets its light from the Sun.',\n",
       " 'The Moo keeps \\nchanging its shape as it moves round the Earth.',\n",
       " 'It spins on its axis in 27.3 \\ndays stars were named after the Edwin Aldrin were the first ones to set their \\nfoot on the Moon on 21 July 1969 They reached the Moon in their space craft named Apollo II.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conjunction ,punctuation,syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a957e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren',\n",
       " ',',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface',\n",
       " '.',\n",
       " 'The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own',\n",
       " '.',\n",
       " 'It',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Sun',\n",
       " '.',\n",
       " 'The',\n",
       " 'Moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'Earth',\n",
       " '.',\n",
       " 'It',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'Edwin',\n",
       " 'Aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'July',\n",
       " '1969',\n",
       " 'They',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'Apollo',\n",
       " 'II',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5445a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren,',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface.',\n",
       " 'The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own.',\n",
       " 'It',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Sun.',\n",
       " 'The',\n",
       " 'Moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'Earth.',\n",
       " 'It',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'Edwin',\n",
       " 'Aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'July',\n",
       " '1969',\n",
       " 'They',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'Apollo',\n",
       " 'II.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# white space tokenizer \n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokens1 = WhitespaceTokenizer().tokenize(text)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01dfa9",
   "metadata": {},
   "source": [
    "### 2. Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd22ad3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren',\n",
       " ',',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface',\n",
       " '.',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own',\n",
       " '.',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'sun',\n",
       " '.',\n",
       " 'the',\n",
       " 'moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.',\n",
       " 'it',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'edwin',\n",
       " 'aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'july',\n",
       " '1969',\n",
       " 'they',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'apollo',\n",
       " 'ii',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase \n",
    "lowercase = [ word.lower() for word in tokens ]\n",
    "lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373fbd9",
   "metadata": {},
   "source": [
    "### 3. Stopwords Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b0e0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1a2474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moon',\n",
       " 'barren',\n",
       " ',',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'water',\n",
       " '.',\n",
       " 'dark']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stop = [ word for word in lowercase if word not in stopword_list]\n",
    "text_without_stop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf07a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'moon', 'is', 'a', 'barren', ',', 'rocky', 'world', 'without', 'air']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9dec52",
   "metadata": {},
   "source": [
    "### 4. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda4f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word  : moon\n",
      "stemmed Word : moon\n",
      "lemmatized Word  : moon\n",
      "****************************************************************************************************\n",
      "Original Word  : barren\n",
      "stemmed Word : bar\n",
      "lemmatized Word  : barren\n",
      "****************************************************************************************************\n",
      "Original Word  : ,\n",
      "stemmed Word : ,\n",
      "lemmatized Word  : ,\n",
      "****************************************************************************************************\n",
      "Original Word  : rocky\n",
      "stemmed Word : rocky\n",
      "lemmatized Word  : rocky\n",
      "****************************************************************************************************\n",
      "Original Word  : world\n",
      "stemmed Word : world\n",
      "lemmatized Word  : world\n",
      "****************************************************************************************************\n",
      "Original Word  : without\n",
      "stemmed Word : without\n",
      "lemmatized Word  : without\n",
      "****************************************************************************************************\n",
      "Original Word  : air\n",
      "stemmed Word : air\n",
      "lemmatized Word  : air\n",
      "****************************************************************************************************\n",
      "Original Word  : water\n",
      "stemmed Word : wat\n",
      "lemmatized Word  : water\n",
      "****************************************************************************************************\n",
      "Original Word  : .\n",
      "stemmed Word : .\n",
      "lemmatized Word  : .\n",
      "****************************************************************************************************\n",
      "Original Word  : dark\n",
      "stemmed Word : dark\n",
      "lemmatized Word  : dark\n",
      "****************************************************************************************************\n",
      "Original Word  : lava\n",
      "stemmed Word : lav\n",
      "lemmatized Word  : lava\n",
      "****************************************************************************************************\n",
      "Original Word  : plain\n",
      "stemmed Word : plain\n",
      "lemmatized Word  : plain\n",
      "****************************************************************************************************\n",
      "Original Word  : surface\n",
      "stemmed Word : surfac\n",
      "lemmatized Word  : surface\n",
      "****************************************************************************************************\n",
      "Original Word  : .\n",
      "stemmed Word : .\n",
      "lemmatized Word  : .\n",
      "****************************************************************************************************\n",
      "Original Word  : moon\n",
      "stemmed Word : moon\n",
      "lemmatized Word  : moon\n",
      "****************************************************************************************************\n",
      "Original Word  : filled\n",
      "stemmed Word : fil\n",
      "lemmatized Word  : filled\n",
      "****************************************************************************************************\n",
      "Original Word  : wit\n",
      "stemmed Word : wit\n",
      "lemmatized Word  : wit\n",
      "****************************************************************************************************\n",
      "Original Word  : craters\n",
      "stemmed Word : crat\n",
      "lemmatized Word  : crater\n",
      "****************************************************************************************************\n",
      "Original Word  : .\n",
      "stemmed Word : .\n",
      "lemmatized Word  : .\n",
      "****************************************************************************************************\n",
      "Original Word  : light\n",
      "stemmed Word : light\n",
      "lemmatized Word  : light\n",
      "****************************************************************************************************\n",
      "Original Word  : .\n",
      "stemmed Word : .\n",
      "lemmatized Word  : .\n",
      "****************************************************************************************************\n",
      "Original Word  : gets\n",
      "stemmed Word : get\n",
      "lemmatized Word  : get\n",
      "****************************************************************************************************\n",
      "Original Word  : light\n",
      "stemmed Word : light\n",
      "lemmatized Word  : light\n",
      "****************************************************************************************************\n",
      "Original Word  : sun\n",
      "stemmed Word : sun\n",
      "lemmatized Word  : sun\n",
      "****************************************************************************************************\n",
      "Original Word  : .\n",
      "stemmed Word : .\n",
      "lemmatized Word  : .\n",
      "****************************************************************************************************\n",
      "Original Word  : moo\n",
      "stemmed Word : moo\n",
      "lemmatized Word  : moo\n",
      "****************************************************************************************************\n",
      "Original Word  : keeps\n",
      "stemmed Word : keep\n",
      "lemmatized Word  : keep\n",
      "****************************************************************************************************\n",
      "Original Word  : changing\n",
      "stemmed Word : chang\n",
      "lemmatized Word  : changing\n",
      "****************************************************************************************************\n",
      "Original Word  : shape\n",
      "stemmed Word : shap\n",
      "lemmatized Word  : shape\n",
      "****************************************************************************************************\n",
      "Original Word  : moves\n",
      "stemmed Word : mov\n",
      "lemmatized Word  : move\n",
      "****************************************************************************************************\n",
      "Original Word  : round\n",
      "stemmed Word : round\n",
      "lemmatized Word  : round\n",
      "****************************************************************************************************\n",
      "Original Word  : earth\n",
      "stemmed Word : ear\n",
      "lemmatized Word  : earth\n",
      "****************************************************************************************************\n",
      "Original Word  : .\n",
      "stemmed Word : .\n",
      "lemmatized Word  : .\n",
      "****************************************************************************************************\n",
      "Original Word  : spins\n",
      "stemmed Word : spin\n",
      "lemmatized Word  : spin\n",
      "****************************************************************************************************\n",
      "Original Word  : axis\n",
      "stemmed Word : ax\n",
      "lemmatized Word  : axis\n",
      "****************************************************************************************************\n",
      "Original Word  : 27.3\n",
      "stemmed Word : 27.3\n",
      "lemmatized Word  : 27.3\n",
      "****************************************************************************************************\n",
      "Original Word  : days\n",
      "stemmed Word : day\n",
      "lemmatized Word  : day\n",
      "****************************************************************************************************\n",
      "Original Word  : stars\n",
      "stemmed Word : star\n",
      "lemmatized Word  : star\n",
      "****************************************************************************************************\n",
      "Original Word  : named\n",
      "stemmed Word : nam\n",
      "lemmatized Word  : named\n",
      "****************************************************************************************************\n",
      "Original Word  : edwin\n",
      "stemmed Word : edwin\n",
      "lemmatized Word  : edwin\n",
      "****************************************************************************************************\n",
      "Original Word  : aldrin\n",
      "stemmed Word : aldrin\n",
      "lemmatized Word  : aldrin\n",
      "****************************************************************************************************\n",
      "Original Word  : first\n",
      "stemmed Word : first\n",
      "lemmatized Word  : first\n",
      "****************************************************************************************************\n",
      "Original Word  : ones\n",
      "stemmed Word : on\n",
      "lemmatized Word  : one\n",
      "****************************************************************************************************\n",
      "Original Word  : set\n",
      "stemmed Word : set\n",
      "lemmatized Word  : set\n",
      "****************************************************************************************************\n",
      "Original Word  : foot\n",
      "stemmed Word : foot\n",
      "lemmatized Word  : foot\n",
      "****************************************************************************************************\n",
      "Original Word  : moon\n",
      "stemmed Word : moon\n",
      "lemmatized Word  : moon\n",
      "****************************************************************************************************\n",
      "Original Word  : 21\n",
      "stemmed Word : 21\n",
      "lemmatized Word  : 21\n",
      "****************************************************************************************************\n",
      "Original Word  : july\n",
      "stemmed Word : july\n",
      "lemmatized Word  : july\n",
      "****************************************************************************************************\n",
      "Original Word  : 1969\n",
      "stemmed Word : 1969\n",
      "lemmatized Word  : 1969\n",
      "****************************************************************************************************\n",
      "Original Word  : reached\n",
      "stemmed Word : reach\n",
      "lemmatized Word  : reached\n",
      "****************************************************************************************************\n",
      "Original Word  : moon\n",
      "stemmed Word : moon\n",
      "lemmatized Word  : moon\n",
      "****************************************************************************************************\n",
      "Original Word  : space\n",
      "stemmed Word : spac\n",
      "lemmatized Word  : space\n",
      "****************************************************************************************************\n",
      "Original Word  : craft\n",
      "stemmed Word : craft\n",
      "lemmatized Word  : craft\n",
      "****************************************************************************************************\n",
      "Original Word  : named\n",
      "stemmed Word : nam\n",
      "lemmatized Word  : named\n",
      "****************************************************************************************************\n",
      "Original Word  : apollo\n",
      "stemmed Word : apollo\n",
      "lemmatized Word  : apollo\n",
      "****************************************************************************************************\n",
      "Original Word  : ii\n",
      "stemmed Word : ii\n",
      "lemmatized Word  : ii\n",
      "****************************************************************************************************\n",
      "Original Word  : .\n",
      "stemmed Word : .\n",
      "lemmatized Word  : .\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in text_without_stop:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    print(f\"Original Word  : {word}\")\n",
    "    print(f\"stemmed Word : {stemmed_word}\")\n",
    "    print(f\"lemmatized Word  : {lemmatized_word}\")\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dbc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter stemmer,snowball stemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0f918",
   "metadata": {},
   "source": [
    "### 5. Contraction Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0762a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I does not like the product'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'doesn't : does not ,didn't : did not }\n",
    "import contractions\n",
    "text = \" I doesn't like the product\"\n",
    "expanded_text = contractions.fix(text)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dd5fc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a, e, i, o, u, A, E, I, O, U I doesn't like the product\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "text = \"â, ê, î, ô, û, Â, Ê, Î, Ô, Û I doesn't like the product\"\n",
    "fixed_text = unidecode(text)\n",
    "fixed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â, ê, î, ô, û, Â, Ê, Î, Ô, Û = accented characters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef631eb",
   "metadata": {},
   "source": [
    "###  Remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a7754f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "192eb8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moon',\n",
       " 'barren',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'water',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'surface',\n",
       " 'moon',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " 'light',\n",
       " 'gets',\n",
       " 'light',\n",
       " 'sun',\n",
       " 'moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'shape',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'earth',\n",
       " 'spins',\n",
       " 'axis',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'named',\n",
       " 'edwin',\n",
       " 'aldrin',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'set',\n",
       " 'foot',\n",
       " 'moon',\n",
       " '21',\n",
       " 'july',\n",
       " '1969',\n",
       " 'reached',\n",
       " 'moon',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'apollo',\n",
       " 'ii']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_punct = [ word for word in text_without_stop if word not in punctuation]\n",
    "text_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549eb9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
